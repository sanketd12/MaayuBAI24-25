import os
from dotenv import load_dotenv
load_dotenv()  # Load environment variables from .env

from typing import List, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import requests  # For calling the Together API
import logging
import langchain_huggingface 
import chromadb
from together import Together

from config import (
    API_CONFIG,
    CORS_CONFIG,
    DB_CONFIG,
    OPENAI_CONFIG,  # Ensure OPENAI_CONFIG pulls in your Together API key from environment variables
    logger
)

try:
    embedding_model = langchain_huggingface.HuggingFaceEmbeddings(model_name=DB_CONFIG["embedding_model"])
    db_client = chromadb.PersistentClient(path=DB_CONFIG["chroma_path"])
    collection = db_client.get_collection("resume_embeddings")
except Exception as e:
    logger.error(str(e))
    raise

# --- LLM Query Function using the Together API ---
def query_llm(prompt: str) -> str:
    """
    Queries the Together API LLM with the given prompt.

    Args:
        prompt: The prompt to send to the LLM.

    Returns:
        str: The text generated by the LLM.
    """
    together_api_url = OPENAI_CONFIG["base_url"]
    api_key = OPENAI_CONFIG["api_key"]  # This should now be loaded from your .env
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "prompt": prompt,
        "max_tokens": 150,
        "temperature": 0.7,
        "top_p": 0.95,
        "n": 1
    }
    client = Together()
    response = client.chat.completions.create(model = "mistralai/Mixtral-8x7B-Instruct-v0.1", messages=[{"role":"user", "content": f"{prompt}"}])
    # if response.status_code == 200:
    #     data = response.json()
    #     # Assumes the response has a "choices" list with the generated text
    #     return data["choices"][0]["text"].strip()

    # else:
    #     raise Exception(f"Together API error: {response.status_code}: {response.text}")
    return response.choices[0].message.content

# --- Initialize FastAPI App ---
app = FastAPI(**API_CONFIG)

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    **CORS_CONFIG
)

# --- Request and Response Models ---
class QueryRequest(BaseModel):
    """Request model for a technology query."""
    query: str = Field(..., description="The technology query regarding job experience")

class QueryResponse(BaseModel):
    """Response model for query results."""
    results: List[str] = Field(..., description="List containing the candidate analysis as structured bullet points")
    count: int = Field(..., description="Number of analysis responses returned")

# --- Health Check Endpoint ---
@app.get("/health")
async def health_check() -> dict:
    """Health check endpoint to verify API status."""
    return {"status": "healthy", "version": API_CONFIG["version"]}

# --- Query Endpoint ---
@app.post("/query", response_model=QueryResponse)
async def query_backend(request: QueryRequest) -> QueryResponse:
    """
    Generate candidate analysis regarding job experience for a particular technology.
    
    This endpoint:
      1. Takes a technology-related query.
      2. Uses the Together API to generate exactly three structured bullet points
         that describe the key job experiences, skills, and qualifications a candidate 
         should possess related to the technology.
    
    Args:
        request: QueryRequest containing the technology query.
        
    Returns:
        QueryResponse containing the analysis generated by the LLM.
        
    Raises:
        HTTPException: If there's an error processing the query.
    """
    logger.info(f"Processing query: {request.query}")
    
    try:
        # Build LLM prompt without candidate resume information
        query_embedding = embedding_model.embed_query(request.query)

        result = collection.query(
            query_embeddings=query_embedding,
            include=["distances"]
            )

        prompt = f"""You are an assistant that analyzes candidate job experience for a particular technology.
Please provide exactly three structured bullet points that outline the key job experiences, skills, and qualifications a candidate should have for the following job description:

description: {request.query}

Here are the candidate descriptions: {result['documents']}

Please also report the similarity: {result['distances']}


Return exactly three clear bullet points.
"""
        # Query the LLM using the Together API
        analysis = query_llm(prompt)
        
        logger.info("LLM candidate analysis generated successfully")
        return QueryResponse(results=[analysis], count=1)
        
    except Exception as e:
        error_msg = f"Error processing query: {str(e)}"
        logger.error(error_msg)
        raise HTTPException(status_code=500, detail=error_msg)

# --- Global Error Handler ---
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Global exception handler for unhandled errors."""
    logger.error(f"Unhandled error: {str(exc)}")
    return {"detail": "An unexpected error occurred"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
